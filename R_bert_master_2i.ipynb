{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hou-jing/Enriching-Pre-trained-Language-Model-with-Entity-Information-for-Relation-Classification/blob/main/R_bert_master_2i.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlRdfgp5W0ao",
        "outputId": "cd19af4d-217f-4d17-af75-e3aab594177a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-10 00:01:39--  https://www.dropbox.com/s/axxqfdte9e92bwz/train.tsv?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/axxqfdte9e92bwz/train.tsv [following]\n",
            "--2022-02-10 00:01:40--  https://www.dropbox.com/s/raw/axxqfdte9e92bwz/train.tsv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc34e74cf97fe163772d9cb757b6.dl.dropboxusercontent.com/cd/0/inline/BfaShNN3ErPU4SjshP4VVYMsO7gMubdcstqY5HD9wDeVXUkxiHCUCsOMWBOVEO91ItsSS6XJJjjB8L2DwKpCa5fuMAIbwB4x3eGti8TzexaFPKCiMOTNi0sQuJAuQ8Pk5T6ImS9KkoFeBT2Y59CVfv3g/file# [following]\n",
            "--2022-02-10 00:01:40--  https://uc34e74cf97fe163772d9cb757b6.dl.dropboxusercontent.com/cd/0/inline/BfaShNN3ErPU4SjshP4VVYMsO7gMubdcstqY5HD9wDeVXUkxiHCUCsOMWBOVEO91ItsSS6XJJjjB8L2DwKpCa5fuMAIbwB4x3eGti8TzexaFPKCiMOTNi0sQuJAuQ8Pk5T6ImS9KkoFeBT2Y59CVfv3g/file\n",
            "Resolving uc34e74cf97fe163772d9cb757b6.dl.dropboxusercontent.com (uc34e74cf97fe163772d9cb757b6.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601f:15::a27d:90f\n",
            "Connecting to uc34e74cf97fe163772d9cb757b6.dl.dropboxusercontent.com (uc34e74cf97fe163772d9cb757b6.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/plain]\n",
            "Saving to: ‘train.tsv?dl=0’\n",
            "\n",
            "train.tsv?dl=0          [ <=>                ]   1.11M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-02-10 00:01:40 (10.4 MB/s) - ‘train.tsv?dl=0’ saved [1159389]\n",
            "\n",
            "--2022-02-10 00:01:41--  https://www.dropbox.com/s/1fv24i278rm0dka/test.tsv?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/1fv24i278rm0dka/test.tsv [following]\n",
            "--2022-02-10 00:01:41--  https://www.dropbox.com/s/raw/1fv24i278rm0dka/test.tsv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc5d383cd912914a93fb53121fd8.dl.dropboxusercontent.com/cd/0/inline/BfaZS6kzCeJ1aU6b6F17pyH4C0pbVoP3fNUQjz0byHjMS04BLNCkA93ZdMvidLrgd39LDGyq0WJ1hdMk6JxK1pvaXi6IFMBQfsa-ZPmHhyt0Af9hkWe-J92s6rmOxhOACXQcSOD1CCU-Gxv8XXqbhhXr/file# [following]\n",
            "--2022-02-10 00:01:41--  https://uc5d383cd912914a93fb53121fd8.dl.dropboxusercontent.com/cd/0/inline/BfaZS6kzCeJ1aU6b6F17pyH4C0pbVoP3fNUQjz0byHjMS04BLNCkA93ZdMvidLrgd39LDGyq0WJ1hdMk6JxK1pvaXi6IFMBQfsa-ZPmHhyt0Af9hkWe-J92s6rmOxhOACXQcSOD1CCU-Gxv8XXqbhhXr/file\n",
            "Resolving uc5d383cd912914a93fb53121fd8.dl.dropboxusercontent.com (uc5d383cd912914a93fb53121fd8.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to uc5d383cd912914a93fb53121fd8.dl.dropboxusercontent.com (uc5d383cd912914a93fb53121fd8.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/plain]\n",
            "Saving to: ‘test.tsv?dl=0’\n",
            "\n",
            "test.tsv?dl=0           [ <=>                ] 383.86K  --.-KB/s    in 0.08s   \n",
            "\n",
            "2022-02-10 00:01:41 (4.84 MB/s) - ‘test.tsv?dl=0’ saved [393072]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.dropbox.com/s/axxqfdte9e92bwz/train.tsv?dl=0\n",
        "\n",
        "!wget https://www.dropbox.com/s/1fv24i278rm0dka/test.tsv?dl=0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJB6HEdngbEx"
      },
      "source": [
        "# 新段落"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBvqh4TGYGEG",
        "outputId": "de570765-ae1e-4cc9-b3a6-60f9c726465d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 5.1 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 50.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 43.0 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 52.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.1)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.4 transformers-4.16.2\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pv6i2IZf5L7"
      },
      "source": [
        "## 尝试2代码"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoDA9XbMAIVh"
      },
      "source": [
        "### 数据预处理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWqCDhZEAHEv",
        "outputId": "14a69086-cc3e-48fb-df24-5e49021268cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
          ]
        }
      ],
      "source": [
        "from random import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "from transformers import BertModel,BertTokenizer\n",
        "# model_name='bert-base-chinese'\n",
        "model_name='bert-base-uncased'\n",
        "# with open('train.tsv?dl=0' , encoding='utf_8', mode='r') as f:\n",
        "#     rel2id = {}\n",
        "#     for line in f:\n",
        "#         line = line.strip().split('\t')\n",
        "#         if line[0] not in rel2id.keys():\n",
        "#             rel2id[line[0]] = len(rel2id)\n",
        "#         else:\n",
        "#             continue\n",
        "rel2id={'Other': 0, 'Cause-Effect(e1,e2)': 1, 'Cause-Effect(e2,e1)': 2, 'Instrument-Agency(e1,e2)': 3, 'Instrument-Agency(e2,e1)': 4, 'Product-Producer(e1,e2)': 5, 'Product-Producer(e2,e1)': 6, 'Content-Container(e1,e2)': 7, 'Content-Container(e2,e1)': 8, 'Entity-Origin(e1,e2)': 9, 'Entity-Origin(e2,e1)': 10, 'Entity-Destination(e1,e2)': 11, 'Entity-Destination(e2,e1)': 12, 'Component-Whole(e1,e2)': 13, 'Component-Whole(e2,e1)': 14, 'Member-Collection(e1,e2)': 15, 'Member-Collection(e2,e1)': 16, 'Message-Topic(e1,e2)': 17, 'Message-Topic(e2,e1)': 18}\n",
        "id2rel={k:v for v,k in rel2id.items()}\n",
        "class preprocess:\n",
        "    def read_file(self,file_name):\n",
        "        with open('%s.tsv?dl=0'%file_name,encoding='utf_8',mode='r') as f:\n",
        "            rel=[]\n",
        "            sentence=[]\n",
        "            max_length=0\n",
        "            for line in f:\n",
        "                line=line.strip().split('\t')\n",
        "                rel.append(line[0])\n",
        "                sent=line[1].replace('</e1>','<e1>').replace('</e2>','<e2>')\n",
        "                sent='CLS '+sent.replace('<e1>','$').replace('<e2>','#')\n",
        "                sentence.append(sent)\n",
        "                if len(sent)>max_length:#max_lenth计算有误，是按单个字母计算的\n",
        "                    max_length=len(sent)\n",
        "\n",
        "            rel_id=[]\n",
        "            for item in rel:\n",
        "                rel_id.append(rel2id[item])\n",
        "\n",
        "        return sentence,rel_id,max_length\n",
        "    def bert_prepro(self,file_name):\n",
        "        self.bert_pre=BertTokenizer.from_pretrained(model_name)\n",
        "        train_sents,train_relid,max_length=self.read_file(file_name)\n",
        "        # print(train_sents[:5],train_relid[:5],rel2id)\n",
        "        inputs=self.bert_pre(train_sents,add_special_tokens=False,padding=True)#max_length=128,truncation=True\n",
        "        inputs_id=inputs['input_ids']\n",
        "        att_mask=inputs['attention_mask']\n",
        "        if type(inputs_id) != torch.Tensor or type(att_mask) != torch.Tensor:\n",
        "            att_mask = torch.tensor(att_mask)  # [item.cpu().detach().numpy() for item in train_mask]\n",
        "            inputs_id = torch.tensor(inputs_id)\n",
        "        print(type(inputs_id), type(att_mask))\n",
        "        return inputs_id,att_mask,train_relid,max_length\n",
        "\n",
        "    def entity_tensor(self):\n",
        "        head_id = self.bert_pre.encode('$', return_tensors='pt', padding=False, add_special_tokens=False)\n",
        "        tail_id = self.bert_pre.encode('#', return_tensors='pt', add_special_tokens=False)\n",
        "        head_id=int(head_id[0])\n",
        "        tail_id=int(tail_id[0])\n",
        "        return head_id,tail_id#plan to find the entity vector\n",
        "    def entity_mask(self,file_name):\n",
        "        inputs_id,att_mask,train_relid,max_length=self.bert_prepro(file_name)\n",
        "        head_id,tail_id=self.entity_tensor()\n",
        "        head_entity_mask = torch.zeros((inputs_id.size()[0], inputs_id.size()[1]))\n",
        "        tail_entity_mask = torch.zeros((inputs_id.size()[0], inputs_id.size()[1]))\n",
        "        for i in range(inputs_id.size(0)):\n",
        "            head_pos = (inputs_id[i].squeeze() == head_id).nonzero().squeeze()\n",
        "            tail_pos = (inputs_id[i].squeeze() == tail_id).nonzero().squeeze()#位置index\n",
        "            for pos in range(int(head_pos[0]) + 1, int(head_pos[1])):\n",
        "                head_entity_mask[i,pos]=1\n",
        "            for pos in range(int(tail_pos[0]) + 1, int(tail_pos[1])):\n",
        "                tail_entity_mask[i, pos] = 1\n",
        "        return head_entity_mask.unsqueeze(-1),tail_entity_mask.unsqueeze(-1)#(增加一个维度）\n",
        "if __name__=='__main__':\n",
        "    file_type=['train','test']\n",
        "    prepro=preprocess()\n",
        "    for ftype in file_type:\n",
        "        if ftype=='train':\n",
        "            train_id,train_mask,train_relid,max_length=prepro.bert_prepro(ftype)\n",
        "            train_head_mask,train_tail_mask=prepro.entity_mask(ftype)\n",
        "        # else:\n",
        "        #     test_id,test_mask,test_relid,max_length=prepro.bert_prepro(ftype)\n",
        "        #     head_mask,tail_mask=prepro.entity_mask(ftype)\n",
        "    head_id,tail_id=prepro.entity_tensor()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def same_seeds(seed):\n",
        "        torch.manual_seed(seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed(seed)\n",
        "            torch.cuda.manual_seed_all(seed)\n",
        "        # np.random.seed(seed)\n",
        "        # random.seed(seed)\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "same_seeds(0)"
      ],
      "metadata": {
        "id": "tPEm03eEXipw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 1"
      ],
      "metadata": {
        "id": "nJf6Fv4WXpfX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuVQIohYfzst",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.label_num = len(rel2id)\n",
        "        self.encode = BertModel.from_pretrained(model_name)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(768, 128),\n",
        "        )\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(384, 64),#128*3\n",
        "            nn.Linear(64, self.label_num),\n",
        "            nn.Softmax(1)\n",
        "        )\n",
        "    def bert_model(self,inputs_id,att_mask,head_mask,tail_mask):\n",
        "        bert_output=self.encode(inputs_id,att_mask)[0]\n",
        "        cls=bert_output[:,0,:]\n",
        "        sent_vector=bert_output[:,1:,:]\n",
        "        head_entity_aver=(sent_vector*head_mask).mean(1)#乘法有错误(batchsize,seq_len,768),(batchsize,seq_len)\n",
        "        tail_entity_aver=(sent_vector*tail_mask).mean(1)\n",
        "        return cls,head_entity_aver,tail_entity_aver\n",
        "    def forward(self,inputs_id,att_mask,head_mask,tail_mask):\n",
        "        cls,head_entity_aver,tail_entity_aver= self.bert_model(inputs_id,att_mask,head_mask,tail_mask)\n",
        "        cls,head_entity_aver,tail_entity_aver=cls.to(device),head_entity_aver.to(device),tail_entity_aver.to(device)\n",
        "        batch_tensor=torch.cat((self.fc(cls),self.fc(head_entity_aver),self.fc(tail_entity_aver)),dim=1)\n",
        "        return self.fc2(batch_tensor)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 2"
      ],
      "metadata": {
        "id": "YhWefHRPXwsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.label_num = len(rel2id)\n",
        "        self.encode = BertModel.from_pretrained(model_name)\n",
        "        self.tanh=nn.Tanh()\n",
        "        self.fc_cls = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(768, 128),\n",
        "        )\n",
        "        self.fc_en1 = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(768, 128),\n",
        "        )\n",
        "        self.fc_en2 = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(768, 128),\n",
        "        )\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            # nn.Linear(384, 64),#128*3\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(384,self.label_num )\n",
        "        )\n",
        "\n",
        "        self.s=nn.Softmax()\n",
        "    def forward(self,inputs_id,att_mask,head_mask,tail_mask):\n",
        "        sent_vector=self.encode(inputs_id,att_mask)[0]\n",
        "        cls=sent_vector[:,0,:]\n",
        "        head_entity_aver=(sent_vector*head_mask).mean(1)#乘法有错误(batchsize,seq_len,768),(batchsize,seq_len)\n",
        "        tail_entity_aver=(sent_vector*tail_mask).mean(1)\n",
        "        cls=self.tanh(cls)\n",
        "        head_entity_aver=self.tanh(head_entity_aver)\n",
        "        tail_entity_aver=self.tanh(tail_entity_aver)\n",
        "        # cls,head_entity_aver,tail_entity_aver=cls.to(device),head_entity_aver.to(device),tail_entity_aver.to(device)\n",
        "        batch_tensor=torch.cat((self.fc_cls(cls),self.fc_en1(head_entity_aver),self.fc_en2(tail_entity_aver)),dim=1)\n",
        "        batch_tensor=self.tanh(batch_tensor)\n",
        "        batch_tensor=self.fc2(batch_tensor)\n",
        "        return batch_tensor"
      ],
      "metadata": {
        "id": "W800fkrbXtjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.label_num = len(rel2id)\n",
        "        self.encode = BertModel.from_pretrained(model_name)\n",
        "        self.tanh=nn.Tanh()\n",
        "        self.fc_cls = nn.Sequential(\n",
        "            # nn.Dropout(0.1),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(768*3, 128*3))\n",
        "        self.fc2 = nn.Sequential(\n",
        "            # nn.Dropout(0.1),\n",
        "            # nn.Linear(384, 64),#128*3\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(384, 1),\n",
        "        )\n",
        "\n",
        "        self.s=nn.Softmax()\n",
        "    def forward(self,inputs_id,att_mask,head_mask,tail_mask):\n",
        "        sent_vector=self.encode(inputs_id,att_mask)[0]\n",
        "        cls=sent_vector[:,0,:]\n",
        "        head_entity_aver=(sent_vector*head_mask).mean(1)#乘法有错误(batchsize,seq_len,768),(batchsize,seq_len)\n",
        "        tail_entity_aver=(sent_vector*tail_mask).mean(1)\n",
        "        cls=self.tanh(cls)\n",
        "        head_entity_aver=self.tanh(head_entity_aver)\n",
        "        tail_entity_aver=self.tanh(tail_entity_aver)\n",
        "        cls,head_entity_aver,tail_entity_aver=cls.to(device),head_entity_aver.to(device),tail_entity_aver.to(device)\n",
        "        batch_tensor=torch.cat((cls,head_entity_aver,tail_entity_aver),dim=1)\n",
        "        batch_tensor=self.fc_cls(batch_tensor)\n",
        "        batch_tensor=self.fc2(batch_tensor)\n",
        "        return batch_tensor"
      ],
      "metadata": {
        "id": "8SVoAe1DKnkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Model()\n",
        "\n",
        "# model.bert_model(torch.from_numpy(np.array(train_id[:4])),torch.from_numpy(np.array(train_mask[:4])))\n",
        "\n",
        "criterier=nn.MSELoss()\n",
        "optim=torch.optim.Adam(model.parameters(),lr=1e-3)\n",
        "\n",
        "from torch.utils.data import TensorDataset,DataLoader\n",
        "train_label=torch.tensor(train_relid)#转为tensor形式\n",
        "print(type(train_label),type(train_mask),type(train_label))\n",
        "if type(train_id)!=torch.Tensor or type(train_mask)!=torch.Tensor:\n",
        "    train_mask=torch.tensor(train_mask)#[item.cpu().detach().numpy() for item in train_mask]\n",
        "    train_id=torch.tensor(train_id)\n",
        "print(type(train_id),type(train_mask),type(train_label))\n",
        "train_set=TensorDataset(train_id,train_mask,train_head_mask,train_tail_mask,train_label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClD0GqH1Xlk7",
        "outputId": "38e54134-ed3f-4509-9033-fcdfd7f8ea9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CCaGl1U0pOIk"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "EPOCHS=5\n",
        "# for epoch in range(EPOCHS):\n",
        "#     step=1\n",
        "#     model.train()\n",
        "#     for i,data in tqdm(enumerate(train_loader)):\n",
        "#         input_id,input_mask,label=data\n",
        "#         pre=model.forward2(input_id,input_mask)\n",
        "#         loss=criterier(pre,label)\n",
        "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.cuda.empty_cache()\n",
        "def train(net,dataset,num_epochs, learning_rate,  batch_size):\n",
        "    i=1\n",
        "    print('执行次数为：{}'.format(i))\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "    net=net.to(device)\n",
        "    net.train()\n",
        "    optimizer =torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
        "    #optimizer = AdamW(net.parameters(), lr=learning_rate)\n",
        "    criterier=nn.CrossEntropyLoss()\n",
        "    for epoch in range(num_epochs):\n",
        "      \n",
        "        train_loss, train_acc = 0, 0\n",
        "        accum_iter=4\n",
        "        step=1\n",
        "        for batch_idx,data in enumerate(tqdm(train_loader)):\n",
        "          text,mask,head_mask,tail_mask,label=data\n",
        "          text,mask,head_mask,tail_mask,label=text.to(device),mask.to(device),head_mask.to(device),tail_mask.to(device),label.to(device)\n",
        "          optimizer.zero_grad()\n",
        "            # with torch.set_grad_enabled(True):\n",
        "          pre = net(text,mask,head_mask,tail_mask)\n",
        "            # label=label.reshape(batch_size,1)\n",
        "            # label=label.to(torch.float)\n",
        "          label=label.squeeze()\n",
        "          label = label.long()\n",
        "          loss = criterier(pre, label)\n",
        "          loss.backward(retain_graph=True)\n",
        "          optimizer.step()\n",
        "          _, predicted = torch.max(pre, 1)\n",
        "          train_acc += (predicted.cpu() == label.cpu()).sum().item()\n",
        "          train_loss += loss.item()\n",
        "          step+=1\n",
        "          if step%100==0:\n",
        "            print('train_epoch|{},acc={},loss={}'.format(epoch,train_acc/(int(step/100)*batch_size*100),train_loss/(int(step/100)*100)))\n",
        "\n",
        "\n",
        "id2rel={k:v for v,k in rel2id.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "be9gZSZjqFfD"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "y_true=train_relid"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 模型训练"
      ],
      "metadata": {
        "id": "b3mmncB-C7Zy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnbPIMnrqGWh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98b9f390-4f53-4954-a382-c4826166dc0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "执行次数为：1\n",
            "train_epoch|0,acc=0.183125,loss=2.7229765963554384\n",
            "train_epoch|0,acc=0.298125,loss=2.5699595767259598\n",
            "train_epoch|0,acc=0.393125,loss=2.414390085140864\n",
            "train_epoch|0,acc=0.46671875,loss=2.275409690737724\n",
            "train_epoch|0,acc=0.519625,loss=2.161467492341995\n",
            "0,0.105375\n",
            "train_epoch|1,acc=0.80375,loss=1.463382865190506\n",
            "train_epoch|1,acc=0.8025,loss=1.4290784987807275\n",
            "train_epoch|1,acc=0.801875,loss=1.3954670498768489\n",
            "train_epoch|1,acc=0.80734375,loss=1.3486649517714977\n",
            "train_epoch|1,acc=0.810375,loss=1.3105094010829925\n",
            "1,0.0825\n",
            "train_epoch|2,acc=0.875625,loss=0.9573589152097702\n",
            "train_epoch|2,acc=0.8778125,loss=0.9350774291157723\n",
            "train_epoch|2,acc=0.879375,loss=0.9153783547878266\n",
            "train_epoch|2,acc=0.8784375,loss=0.9037987418472767\n",
            "train_epoch|2,acc=0.8755,loss=0.8972366044521332\n",
            "2,0.08675\n",
            "train_epoch|3,acc=0.895625,loss=0.7054354539513588\n",
            "train_epoch|3,acc=0.9128125,loss=0.6613687165081501\n",
            "train_epoch|3,acc=0.9122916666666666,loss=0.6592493134737015\n",
            "train_epoch|3,acc=0.91140625,loss=0.6529287548363208\n",
            "train_epoch|3,acc=0.91525,loss=0.6333527263402939\n",
            "3,0.086875\n",
            "train_epoch|4,acc=0.943125,loss=0.4625596676766872\n",
            "train_epoch|4,acc=0.93875,loss=0.47929693579673766\n",
            "train_epoch|4,acc=0.9366666666666666,loss=0.4846216577291489\n",
            "train_epoch|4,acc=0.938125,loss=0.47564621340483426\n",
            "train_epoch|4,acc=0.938375,loss=0.4690301252603531\n",
            "4,0.079\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "# for epoch in range(EPOCHS):\n",
        "#     step=1\n",
        "#     model.train()\n",
        "#     for i,data in tqdm(enumerate(train_loader)):\n",
        "#         input_id,input_mask,label=data\n",
        "#         pre=model.forward2(input_id,input_mask)\n",
        "#         loss=criterier(pre,label)\n",
        "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "def train(net,dataset,num_epochs, learning_rate,  batch_size):\n",
        "    i=1\n",
        "    print('执行次数为：{}'.format(i))\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "    t_total = len(train_loader) //num_epochs\n",
        "    optimizer =torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
        "    # scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=t_total)\n",
        "    \n",
        "    net=net.to(device)\n",
        "    net.train()   \n",
        "    #optimizer = AdamW(net.parameters(), lr=learning_rate)\n",
        "    criterier=nn.CrossEntropyLoss()\n",
        "    # criterier=nn.MSELoss()\n",
        "    for epoch in range(num_epochs):\n",
        "      predict=[]\n",
        "      train_loss, train_acc = 0, 0\n",
        "      accum_iter=4\n",
        "      step=1\n",
        "      for batch_idx,data in enumerate(train_loader):\n",
        "        text,mask,head_mask,tail_mask,label=data\n",
        "        text,mask,head_mask,tail_mask,label=text.to(device),mask.to(device),head_mask.to(device),tail_mask.to(device),label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "            # with torch.set_grad_enabled(True):\n",
        "        pre = net(text,mask,head_mask,tail_mask)\n",
        "            # label=label.reshape(batch_size,1)\n",
        "            # label=label.to(torch.float)\n",
        "        label=label.squeeze()\n",
        "        label = label.long()\n",
        "        # label = label.float()\n",
        "        loss = criterier(pre, label)\n",
        "        loss.backward()\n",
        "  \n",
        "        optimizer.step()\n",
        "        # scheduler.step()\n",
        "        _, predicted = torch.max(pre, 1)\n",
        "        predict.extend(predicted.cpu().tolist())\n",
        "        train_acc += (predicted.cpu() == label.cpu()).sum().item()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        step+=1\n",
        "        if step%100==0:\n",
        "          print('train_epoch|{},acc={},loss={}'.format(epoch,train_acc/(int(step/100)*batch_size*100),train_loss/(int(step/100)*100)))\n",
        "      acc1=accuracy_score(y_true,predict,normalize=True)\n",
        "\n",
        "      print('{},{}'.format(epoch,acc1))\n",
        "\n",
        "id2rel={k:v for v,k in rel2id.items()}\n",
        "EPOCHS=5\n",
        "train(model,train_set,EPOCHS,learning_rate=2e-5,batch_size=16)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UGSVzTUh85Rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vny875Pu3rQi"
      },
      "outputs": [],
      "source": [
        "EPOCHS=20\n",
        "train(model,train_set,EPOCHS,learning_rate=1e-3,batch_size=16)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZqURiWzxBir7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### test 数据预处理"
      ],
      "metadata": {
        "id": "F5UWPCcxBruE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57e3fd48-f6cf-4014-b234-fb07fa3b51c4",
        "id": "3PfBu0y8Bo6G"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class preprocess:\n",
        "    def read_file(self,file_name):\n",
        "        with open('%s.tsv?dl=0'%file_name,encoding='utf_8',mode='r') as f:\n",
        "            rel=[]\n",
        "            sentence=[]\n",
        "            max_length=0\n",
        "            for line in f:\n",
        "                line=line.strip().split('\t')\n",
        "                rel.append(line[0])\n",
        "                sent=line[1].replace('</e1>','<e1>').replace('</e2>','<e2>')\n",
        "                sent='CLS '+sent.replace('<e1>','$').replace('<e2>','#')\n",
        "                sentence.append(sent)\n",
        "                if len(sent)>max_length:#max_lenth计算有误，是按单个字母计算的\n",
        "                    max_length=len(sent)\n",
        "\n",
        "            rel_id=[]\n",
        "            for item in rel:\n",
        "                rel_id.append(rel2id[item])\n",
        "\n",
        "        return sentence,rel_id,max_length\n",
        "    def bert_prepro(self,file_name):\n",
        "        self.bert_pre=BertTokenizer.from_pretrained(model_name)\n",
        "        train_sents,train_relid,max_length=self.read_file(file_name)\n",
        "        # print(train_sents[:5],train_relid[:5],rel2id)\n",
        "        inputs=self.bert_pre(train_sents,add_special_tokens=False,padding=True)#max_length=128,truncation=True\n",
        "        inputs_id=inputs['input_ids']\n",
        "        att_mask=inputs['attention_mask']\n",
        "        if type(inputs_id) != torch.Tensor or type(att_mask) != torch.Tensor:\n",
        "            att_mask = torch.tensor(att_mask)  # [item.cpu().detach().numpy() for item in train_mask]\n",
        "            inputs_id = torch.tensor(inputs_id)\n",
        "        print(type(inputs_id), type(att_mask))\n",
        "        return inputs_id,att_mask,train_relid,max_length\n",
        "\n",
        "    def entity_tensor(self):\n",
        "        head_id = self.bert_pre.encode('$', return_tensors='pt', padding=False, add_special_tokens=False)\n",
        "        tail_id = self.bert_pre.encode('#', return_tensors='pt', add_special_tokens=False)\n",
        "        head_id=int(head_id[0])\n",
        "        tail_id=int(tail_id[0])\n",
        "        return head_id,tail_id#plan to find the entity vector\n",
        "    def entity_mask(self,file_name):\n",
        "        inputs_id,att_mask,train_relid,max_length=self.bert_prepro(file_name)\n",
        "        head_id,tail_id=self.entity_tensor()\n",
        "        head_entity_mask = torch.zeros((inputs_id.size()[0], inputs_id.size()[1]))\n",
        "        tail_entity_mask = torch.zeros((inputs_id.size()[0], inputs_id.size()[1]))\n",
        "        for i in range(inputs_id.size(0)):\n",
        "            head_pos = (inputs_id[i].squeeze() == head_id).nonzero().squeeze()\n",
        "            tail_pos = (inputs_id[i].squeeze() == tail_id).nonzero().squeeze()#位置index\n",
        "            for pos in range(int(head_pos[0]) + 1, int(head_pos[1])):\n",
        "                head_entity_mask[i,pos]=1\n",
        "            for pos in range(int(tail_pos[0]) + 1, int(tail_pos[1])):\n",
        "                tail_entity_mask[i, pos] = 1\n",
        "        return head_entity_mask.unsqueeze(-1),tail_entity_mask.unsqueeze(-1)#(增加一个维度）\n",
        "if __name__=='__main__':\n",
        "    file_type=['test']\n",
        "    prepro=preprocess()\n",
        "    for ftype in file_type:\n",
        "        if ftype=='train':\n",
        "            train_id,train_mask,train_relid,max_length=prepro.bert_prepro(ftype)\n",
        "            train_head_mask,train_tail_mask=prepro.entity_mask(ftype)\n",
        "        else:\n",
        "            test_id,test_mask,test_relid,max_length=prepro.bert_prepro(ftype)\n",
        "            head_mask,tail_mask=prepro.entity_mask(ftype)\n",
        "    head_id,tail_id=prepro.entity_tensor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI2JUWa2XCUH"
      },
      "source": [
        "### 模型测试"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def model_test(net,dataset,num_epochs, learning_rate,  batch_size):\n",
        "    net.to(device)\n",
        "    net.eval()\n",
        "    # rel_list = []\n",
        "    correct=0\n",
        "    total=0\n",
        "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    with torch.no_grad():\n",
        "        result=[]\n",
        "        for batch_idx, data in enumerate(tqdm(train_loader)):\n",
        "            text,mask,head_mask,tail_mask=data\n",
        "            text,mask,head_mask,tail_mask=text.to(device),mask.to(device),head_mask.to(device),tail_mask.to(device)\n",
        "            pre = net(text,mask,head_mask,tail_mask)\n",
        "            # label=label.reshape(batch_size,1)\n",
        "            # label=label.to(torch.float)\n",
        "            _, predicted = torch.max(pre, 1)\n",
        "            result.extend(predicted.cpu().detach().tolist())\n",
        "        #     correct += (predicted.cpu() == label.cpu()).sum().item()\n",
        "        # print('test_acc={}'.format(correct /len(train_loader)*4))\n",
        "    # rel_list.append(id2rel[result])\n",
        "    return result\n",
        "train_set=TensorDataset(test_id,test_mask,head_mask,tail_mask)\n",
        "rel_list=model_test(model,train_set,EPOCHS,learning_rate=1e-3,batch_size=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FpHxg6KBYeF",
        "outputId": "3be49a83-67bd-4bbb-dea9-4c067b0387a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 680/680 [00:11<00:00, 60.72it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 准确率和召回率计算"
      ],
      "metadata": {
        "id": "-VL07DLeIIvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(rel_list[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IL7a9B8EnJ9",
        "outputId": "80a99bbd-a711-48ef-ba64-fdc5ed2e4b38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[17, 6, 4, 11, 2, 13, 0, 0, 0, 17]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_relid[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zERPGkzEEwbm",
        "outputId": "ddeb2f6e-58a1-474b-a6ed-b9c579863e44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[17, 6, 4, 11, 2, 13, 5, 16, 13, 17]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc1=accuracy_score(test_relid,rel_list,normalize=True)\n",
        "print(acc1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-DhC_UvE3jd",
        "outputId": "0f10c49e-c58d-4dad-8fe6-89fdfa4a9516"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8292234081707766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "mfAgDZPWHEBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import recall_score\n",
        "\n",
        "rel1=recall_score(test_relid,rel_list,average='micro')\n",
        "print(rel1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHujh0D_FTuX",
        "outputId": "09c8f4be-bfe3-4ae0-915f-e027e05c4460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8292234081707766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pre, rec, f1, sup = precision_recall_fscore_support(test_relid,rel_list)\n",
        "print(\"precision:\", pre, \"\\nrecall:\", rec, \"\\nf1-score:\", f1, \"\\nsupport:\", sup)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbUlvK8LHJeV",
        "outputId": "94ae4477-75a4-4e26-fb83-e22a77369b97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision: [0.65270936 0.92753623 0.90547264 0.8        0.68156425 0.83809524\n",
            " 0.82170543 0.92537313 0.88571429 0.87019231 0.95348837 0.9\n",
            " 0.         0.91724138 0.7615894  0.78571429 0.85321101 0.90140845\n",
            " 0.77966102] \n",
            "recall: [0.58370044 0.95522388 0.93814433 0.54545455 0.91044776 0.81481481\n",
            " 0.86178862 0.81045752 0.79487179 0.85781991 0.87234043 0.95876289\n",
            " 0.         0.82098765 0.76666667 0.6875     0.92537313 0.91428571\n",
            " 0.90196078] \n",
            "f1-score: [0.61627907 0.94117647 0.92151899 0.64864865 0.77955272 0.82629108\n",
            " 0.84126984 0.8641115  0.83783784 0.86396181 0.91111111 0.92845258\n",
            " 0.         0.86644951 0.7641196  0.73333333 0.88782816 0.90780142\n",
            " 0.83636364] \n",
            "support: [454 134 194  22 134 108 123 153  39 211  47 291   1 162 150  32 201 210\n",
            "  51]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bert_NO_SEQ\n",
        "vector中学习speical token的encoding_"
      ],
      "metadata": {
        "id": "Q2JeHkKtmjWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#第二种配置是丢弃特定的单独标记（即“$”和“#”）\n",
        "class preprocess:\n",
        "    def read_file(self,file_name):\n",
        "        with open('%s.tsv?dl=0'%file_name,encoding='utf_8',mode='r') as f:\n",
        "            rel=[]\n",
        "            sentence=[]\n",
        "            max_length=0\n",
        "            for line in f:\n",
        "                line=line.strip().split('\t')\n",
        "                rel.append(line[0])\n",
        "                sent=line[1].replace('</e1>','<e1>').replace('</e2>','<e2>')\n",
        "                sent='CLS '+sent.replace('<e1>','$').replace('<e2>','#')\n",
        "                sentence.append(sent)\n",
        "                if len(sent)>max_length:#max_lenth计算有误，是按单个字母计算的\n",
        "                    max_length=len(sent)\n",
        "\n",
        "            rel_id=[]\n",
        "            for item in rel:\n",
        "                rel_id.append(rel2id[item])\n",
        "\n",
        "        return sentence,rel_id,max_length\n",
        "    def bert_prepro(self,file_name):\n",
        "        self.bert_pre=BertTokenizer.from_pretrained(model_name)\n",
        "        train_sents,train_relid,max_length=self.read_file(file_name)\n",
        "        # print(train_sents[:5],train_relid[:5],rel2id)\n",
        "        inputs=self.bert_pre(train_sents,add_special_tokens=False,padding=True)#max_length=128,truncation=True\n",
        "        inputs_id=inputs['input_ids']\n",
        "        att_mask=inputs['attention_mask']\n",
        "        if type(inputs_id) != torch.Tensor or type(att_mask) != torch.Tensor:\n",
        "            att_mask = torch.tensor(att_mask)  # [item.cpu().detach().numpy() for item in train_mask]\n",
        "            inputs_id = torch.tensor(inputs_id)\n",
        "        print(type(inputs_id), type(att_mask))\n",
        "        return inputs_id,att_mask,train_relid,max_length\n",
        "\n",
        "    def entity_tensor(self):\n",
        "        head_id = self.bert_pre.encode('$', return_tensors='pt', padding=False, add_special_tokens=False)\n",
        "        tail_id = self.bert_pre.encode('#', return_tensors='pt', add_special_tokens=False)\n",
        "        head_id=int(head_id[0])\n",
        "        tail_id=int(tail_id[0])\n",
        "        return head_id,tail_id#plan to find the entity vector\n",
        "    def entity_mask(self,file_name):\n",
        "        inputs_id,att_mask,train_relid,max_length=self.bert_prepro(file_name)\n",
        "        head_id,tail_id=self.entity_tensor()\n",
        "        head_entity_mask = torch.zeros((inputs_id.size()[0], inputs_id.size()[1]))\n",
        "        tail_entity_mask = torch.zeros((inputs_id.size()[0], inputs_id.size()[1]))\n",
        "        for i in range(inputs_id.size(0)):\n",
        "            head_pos = (inputs_id[i].squeeze() == head_id).nonzero().squeeze()\n",
        "            tail_pos = (inputs_id[i].squeeze() == tail_id).nonzero().squeeze()#位置index\n",
        "            for pos in range(int(head_pos[0]), int(head_pos[1])+1):\n",
        "                head_entity_mask[i,pos]=1\n",
        "            for pos in range(int(tail_pos[0]), int(tail_pos[1])+1):\n",
        "                tail_entity_mask[i, pos] = 1\n",
        "        return head_entity_mask.unsqueeze(-1),tail_entity_mask.unsqueeze(-1)#(增加一个维度）\n",
        "if __name__=='__main__':\n",
        "    file_type=['train','test']\n",
        "    prepro=preprocess()\n",
        "    for ftype in file_type:\n",
        "        if ftype=='train':\n",
        "            train_id,train_mask,train_relid,max_length=prepro.bert_prepro(ftype)\n",
        "            train_head_mask,train_tail_mask=prepro.entity_mask(ftype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDvuBQWrmi6l",
        "outputId": "7ccdb349-08d4-4022-cc2f-204b62f043a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### model"
      ],
      "metadata": {
        "id": "udESKR6Dpwp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.label_num = len(rel2id)\n",
        "        self.encode = BertModel.from_pretrained(model_name)\n",
        "        self.tanh=nn.Tanh()\n",
        "        self.fc_cls = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(768, 128),\n",
        "        )\n",
        "        self.fc_en1 = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(768, 128),\n",
        "        )\n",
        "        self.fc_en2 = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(768, 128),\n",
        "        )\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            # nn.Linear(384, 64),#128*3\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(384,self.label_num )\n",
        "        )\n",
        "\n",
        "        self.s=nn.Softmax()\n",
        "    def forward(self,inputs_id,att_mask,head_mask,tail_mask):\n",
        "        sent_vector=self.encode(inputs_id,att_mask)[0]\n",
        "        cls=sent_vector[:,0,:]\n",
        "        head_entity_aver=(sent_vector*head_mask).mean(1)#乘法有错误(batchsize,seq_len,768),(batchsize,seq_len)\n",
        "        tail_entity_aver=(sent_vector*tail_mask).mean(1)\n",
        "        cls=self.tanh(cls)\n",
        "        head_entity_aver=self.tanh(head_entity_aver)\n",
        "        tail_entity_aver=self.tanh(tail_entity_aver)\n",
        "        # cls,head_entity_aver,tail_entity_aver=cls.to(device),head_entity_aver.to(device),tail_entity_aver.to(device)\n",
        "        batch_tensor=torch.cat((self.fc_cls(cls),self.fc_en1(head_entity_aver),self.fc_en2(tail_entity_aver)),dim=1)\n",
        "        batch_tensor=self.tanh(batch_tensor)\n",
        "        batch_tensor=self.fc2(batch_tensor)\n",
        "        return batch_tensor"
      ],
      "metadata": {
        "id": "AxNKyjgapvYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Model()\n",
        "\n",
        "# model.bert_model(torch.from_numpy(np.array(train_id[:4])),torch.from_numpy(np.array(train_mask[:4])))\n",
        "\n",
        "criterier=nn.MSELoss()\n",
        "optim=torch.optim.Adam(model.parameters(),lr=1e-3)\n",
        "\n",
        "from torch.utils.data import TensorDataset,DataLoader\n",
        "train_label=torch.tensor(train_relid)#转为tensor形式\n",
        "print(type(train_label),type(train_mask),type(train_label))\n",
        "if type(train_id)!=torch.Tensor or type(train_mask)!=torch.Tensor:\n",
        "    train_mask=torch.tensor(train_mask)#[item.cpu().detach().numpy() for item in train_mask]\n",
        "    train_id=torch.tensor(train_id)\n",
        "print(type(train_id),type(train_mask),type(train_label))\n",
        "train_set=TensorDataset(train_id,train_mask,train_head_mask,train_tail_mask,train_label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71517378-4843-4e53-d213-db7750b45e52",
        "id": "0-rJFWW4p26a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train"
      ],
      "metadata": {
        "id": "zYQX2sjEp3fa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3937bcc6-7196-4c28-dfdc-743024c03a98",
        "id": "yhcNReaTp8Ps"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "执行次数为：1\n",
            "train_epoch|0,acc=0.166875,loss=2.766641778945923\n",
            "train_epoch|0,acc=0.2128125,loss=2.6897992134094237\n",
            "train_epoch|0,acc=0.29270833333333335,loss=2.55629812280337\n",
            "train_epoch|0,acc=0.36125,loss=2.4308526581525802\n",
            "train_epoch|0,acc=0.425375,loss=2.3031116523742674\n",
            "0,0.124625\n",
            "train_epoch|1,acc=0.7375,loss=1.594305429458618\n",
            "train_epoch|1,acc=0.7559375,loss=1.5239644068479539\n",
            "train_epoch|1,acc=0.768125,loss=1.459875106215477\n",
            "train_epoch|1,acc=0.77390625,loss=1.4165464161336423\n",
            "train_epoch|1,acc=0.7835,loss=1.3669062204360962\n",
            "1,0.078\n",
            "train_epoch|2,acc=0.869375,loss=0.9447288227081299\n",
            "train_epoch|2,acc=0.869375,loss=0.934832047522068\n",
            "train_epoch|2,acc=0.8670833333333333,loss=0.9180077483256658\n",
            "train_epoch|2,acc=0.86515625,loss=0.9067925771325827\n",
            "train_epoch|2,acc=0.864375,loss=0.8848348079919816\n",
            "2,0.0855\n",
            "train_epoch|3,acc=0.90125,loss=0.6531433871388436\n",
            "train_epoch|3,acc=0.905625,loss=0.6389143994450569\n",
            "train_epoch|3,acc=0.9095833333333333,loss=0.6234542191028595\n",
            "train_epoch|3,acc=0.913125,loss=0.6099259344488382\n",
            "train_epoch|3,acc=0.911625,loss=0.6025498881936073\n",
            "3,0.08425\n",
            "train_epoch|4,acc=0.925625,loss=0.47131198659539225\n",
            "train_epoch|4,acc=0.9290625,loss=0.46373039469122884\n",
            "train_epoch|4,acc=0.93375,loss=0.45251257573564846\n",
            "train_epoch|4,acc=0.9346875,loss=0.4494447649270296\n",
            "train_epoch|4,acc=0.936,loss=0.4401601472198963\n",
            "4,0.081625\n",
            "train_epoch|5,acc=0.948125,loss=0.330617812871933\n",
            "train_epoch|5,acc=0.9540625,loss=0.32896969236433504\n",
            "train_epoch|5,acc=0.953125,loss=0.32521500227351985\n",
            "train_epoch|5,acc=0.95203125,loss=0.3236287020146847\n",
            "train_epoch|5,acc=0.951875,loss=0.32213364671170713\n",
            "5,0.08\n",
            "train_epoch|6,acc=0.950625,loss=0.27341384656727313\n",
            "train_epoch|6,acc=0.9565625,loss=0.26693652868270873\n",
            "train_epoch|6,acc=0.9583333333333334,loss=0.26531742359201116\n",
            "train_epoch|6,acc=0.95953125,loss=0.26211197348311543\n",
            "train_epoch|6,acc=0.96075,loss=0.25543424862623215\n",
            "6,0.085375\n",
            "train_epoch|7,acc=0.955,loss=0.22392183795571327\n",
            "train_epoch|7,acc=0.961875,loss=0.2198198386654258\n",
            "train_epoch|7,acc=0.965625,loss=0.211257237692674\n",
            "train_epoch|7,acc=0.968125,loss=0.202274339068681\n",
            "train_epoch|7,acc=0.96825,loss=0.20029396712034941\n",
            "7,0.080875\n",
            "train_epoch|8,acc=0.971875,loss=0.1529091639816761\n",
            "train_epoch|8,acc=0.97625,loss=0.15431049266830088\n",
            "train_epoch|8,acc=0.975,loss=0.1607133403296272\n",
            "train_epoch|8,acc=0.9721875,loss=0.17230547162704168\n",
            "train_epoch|8,acc=0.972625,loss=0.16896145813167096\n",
            "8,0.082625\n",
            "train_epoch|9,acc=0.970625,loss=0.13376702204346658\n",
            "train_epoch|9,acc=0.9746875,loss=0.1365569807961583\n",
            "train_epoch|9,acc=0.975625,loss=0.13983356601869065\n",
            "train_epoch|9,acc=0.9775,loss=0.135381347858347\n",
            "train_epoch|9,acc=0.976625,loss=0.14068310565873982\n",
            "9,0.081875\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "# for epoch in range(EPOCHS):\n",
        "#     step=1\n",
        "#     model.train()\n",
        "#     for i,data in tqdm(enumerate(train_loader)):\n",
        "#         input_id,input_mask,label=data\n",
        "#         pre=model.forward2(input_id,input_mask)\n",
        "#         loss=criterier(pre,label)\n",
        "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "def train(net,dataset,num_epochs, learning_rate,  batch_size):\n",
        "    i=1\n",
        "    print('执行次数为：{}'.format(i))\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "    t_total = len(train_loader) //num_epochs\n",
        "    optimizer =torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
        "    # scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=t_total)\n",
        "    \n",
        "    net=net.to(device)\n",
        "    net.train()   \n",
        "    #optimizer = AdamW(net.parameters(), lr=learning_rate)\n",
        "    criterier=nn.CrossEntropyLoss()\n",
        "    # criterier=nn.MSELoss()\n",
        "    for epoch in range(num_epochs):\n",
        "      predict=[]\n",
        "      train_loss, train_acc = 0, 0\n",
        "      accum_iter=4\n",
        "      step=1\n",
        "      for batch_idx,data in enumerate(train_loader):\n",
        "        text,mask,head_mask, tail_mask, label=data\n",
        "        text,mask,head_mask,tail_mask,label=text.to(device),mask.to(device),head_mask.to(device),tail_mask.to(device),label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "            # with torch.set_grad_enabled(True):\n",
        "        pre = net(text,mask,head_mask,tail_mask)\n",
        "            # label=label.reshape(batch_size,1)\n",
        "            # label=label.to(torch.float)\n",
        "        label=label.squeeze()\n",
        "        label = label.long()\n",
        "        # label = label.float()\n",
        "        loss = criterier(pre, label)\n",
        "        loss.backward()\n",
        "  \n",
        "        optimizer.step()\n",
        "        # scheduler.step()\n",
        "        _, predicted = torch.max(pre, 1)\n",
        "        predict.extend(predicted.cpu().tolist())\n",
        "        train_acc += (predicted.cpu() == label.cpu()).sum().item()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        step+=1\n",
        "        if step%100==0:\n",
        "          print('train_epoch|{},acc={},loss={}'.format(epoch,train_acc/(int(step/100)*batch_size*100),train_loss/(int(step/100)*100)))\n",
        "      acc1=accuracy_score(y_true,predict,normalize=True)\n",
        "\n",
        "      print('{},{}'.format(epoch,acc1))\n",
        "\n",
        "id2rel={k:v for v,k in rel2id.items()}\n",
        "EPOCHS=10\n",
        "train(model,train_set,EPOCHS,learning_rate=2e-5,batch_size=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### test"
      ],
      "metadata": {
        "id": "OgDejafmqXBW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcfffaa9-7b21-49d3-f3c0-e4698e02bee7",
        "id": "rwXNXR_qqRI1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class preprocess:\n",
        "    def read_file(self,file_name):\n",
        "        with open('%s.tsv?dl=0'%file_name,encoding='utf_8',mode='r') as f:\n",
        "            rel=[]\n",
        "            sentence=[]\n",
        "            max_length=0\n",
        "            for line in f:\n",
        "                line=line.strip().split('\t')\n",
        "                rel.append(line[0])\n",
        "                sent=line[1].replace('</e1>','<e1>').replace('</e2>','<e2>')\n",
        "                sent='CLS '+sent.replace('<e1>','$').replace('<e2>','#')\n",
        "                sentence.append(sent)\n",
        "                if len(sent)>max_length:#max_lenth计算有误，是按单个字母计算的\n",
        "                    max_length=len(sent)\n",
        "\n",
        "            rel_id=[]\n",
        "            for item in rel:\n",
        "                rel_id.append(rel2id[item])\n",
        "\n",
        "        return sentence,rel_id,max_length\n",
        "    def bert_prepro(self,file_name):\n",
        "        self.bert_pre=BertTokenizer.from_pretrained(model_name)\n",
        "        train_sents,train_relid,max_length=self.read_file(file_name)\n",
        "        # print(train_sents[:5],train_relid[:5],rel2id)\n",
        "        inputs=self.bert_pre(train_sents,add_special_tokens=False,padding=True)#max_length=128,truncation=True\n",
        "        inputs_id=inputs['input_ids']\n",
        "        att_mask=inputs['attention_mask']\n",
        "        if type(inputs_id) != torch.Tensor or type(att_mask) != torch.Tensor:\n",
        "            att_mask = torch.tensor(att_mask)  # [item.cpu().detach().numpy() for item in train_mask]\n",
        "            inputs_id = torch.tensor(inputs_id)\n",
        "        print(type(inputs_id), type(att_mask))\n",
        "        return inputs_id,att_mask,train_relid,max_length\n",
        "\n",
        "    def entity_tensor(self):\n",
        "        head_id = self.bert_pre.encode('$', return_tensors='pt', padding=False, add_special_tokens=False)\n",
        "        tail_id = self.bert_pre.encode('#', return_tensors='pt', add_special_tokens=False)\n",
        "        head_id=int(head_id[0])\n",
        "        tail_id=int(tail_id[0])\n",
        "        return head_id,tail_id#plan to find the entity vector\n",
        "    def entity_mask(self,file_name):\n",
        "        inputs_id,att_mask,train_relid,max_length=self.bert_prepro(file_name)\n",
        "        head_id,tail_id=self.entity_tensor()\n",
        "        head_entity_mask = torch.zeros((inputs_id.size()[0], inputs_id.size()[1]))\n",
        "        tail_entity_mask = torch.zeros((inputs_id.size()[0], inputs_id.size()[1]))\n",
        "        for i in range(inputs_id.size(0)):\n",
        "            head_pos = (inputs_id[i].squeeze() == head_id).nonzero().squeeze()\n",
        "            tail_pos = (inputs_id[i].squeeze() == tail_id).nonzero().squeeze()#位置index\n",
        "            for pos in range(int(head_pos[0]) + 1, int(head_pos[1])):\n",
        "                head_entity_mask[i,pos]=1\n",
        "            for pos in range(int(tail_pos[0]) + 1, int(tail_pos[1])):\n",
        "                tail_entity_mask[i, pos] = 1\n",
        "        return head_entity_mask.unsqueeze(-1),tail_entity_mask.unsqueeze(-1)#(增加一个维度）\n",
        "if __name__=='__main__':\n",
        "    file_type=['test']\n",
        "    prepro=preprocess()\n",
        "    for ftype in file_type:\n",
        "        if ftype=='train':\n",
        "            train_id,train_mask,train_relid,max_length=prepro.bert_prepro(ftype)\n",
        "            train_head_mask,train_tail_mask=prepro.entity_mask(ftype)\n",
        "        else:\n",
        "            test_id,test_mask,test_relid,max_length=prepro.bert_prepro(ftype)\n",
        "            head_mask,tail_mask=prepro.entity_mask(ftype)\n",
        "    head_id,tail_id=prepro.entity_tensor()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def model_test(net,dataset,num_epochs, learning_rate,  batch_size):\n",
        "    net.to(device)\n",
        "    net.eval()\n",
        "    # rel_list = []\n",
        "    correct=0\n",
        "    total=0\n",
        "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    with torch.no_grad():\n",
        "        result=[]\n",
        "        for batch_idx, data in enumerate(tqdm(train_loader)):\n",
        "            text,mask,head_mask,tail_mask=data\n",
        "            text,mask,head_mask,tail_mask=text.to(device),mask.to(device),head_mask.to(device),tail_mask.to(device)\n",
        "            pre = net(text,mask,head_mask,tail_mask)\n",
        "            # label=label.reshape(batch_size,1)\n",
        "            # label=label.to(torch.float)\n",
        "            _, predicted = torch.max(pre, 1)\n",
        "            result.extend(predicted.cpu().detach().tolist())\n",
        "        #     correct += (predicted.cpu() == label.cpu()).sum().item()\n",
        "        # print('test_acc={}'.format(correct /len(train_loader)*4))\n",
        "    # rel_list.append(id2rel[result])\n",
        "    return result\n",
        "train_set=TensorDataset(test_id,test_mask,head_mask,tail_mask)\n",
        "rel_list=model_test(model,train_set,EPOCHS,learning_rate=1e-3,batch_size=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca7f34c4-903d-467f-fa68-09ba1f48e666",
        "id": "V95dvH1VqZFk"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 680/680 [00:10<00:00, 62.18it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_relid[:10])\n",
        "rel_list[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqZsqO80qmj1",
        "outputId": "4620b68b-ac9f-4e13-d71f-2dae07bc1c5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[17, 6, 4, 11, 2, 13, 5, 16, 13, 17]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[17, 6, 4, 11, 2, 13, 0, 0, 13, 17]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### acc&&F1"
      ],
      "metadata": {
        "id": "S0vmJz34qnfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "acc1=accuracy_score(test_relid,rel_list,normalize=True)\n",
        "print(acc1)\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "rel1=recall_score(test_relid,rel_list,average='micro')\n",
        "print(rel1)"
      ],
      "metadata": {
        "id": "6nl9Va2cqfUO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92fda7c2-99ad-46dc-ab94-71270eeb49d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8244387191755613\n",
            "0.8244387191755613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rel1=recall_score(test_relid,rel_list,average='macro')\n",
        "print(rel1)"
      ],
      "metadata": {
        "id": "AOngSQT75dVK",
        "outputId": "76e39a14-3fad-4361-dd4c-624007d4edee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7801739304822566\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DVzrP4P-nOG2"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "R_bert_master_2i.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMVndfcrjPJA17xCR/+3i1+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}